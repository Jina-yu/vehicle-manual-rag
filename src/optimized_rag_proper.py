"""
optimized_rag_proper.py
LLMì„ ìœ ì§€í•˜ë©´ì„œ ì‘ë‹µì‹œê°„ì„ ê°œì„ í•˜ëŠ” ì˜¬ë°”ë¥¸ ë°©ë²•
ëª©í‘œ: 1ì´ˆ ì´ë‚´ (í˜„ì‹¤ì  ëª©í‘œ)
"""

from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from typing import List, Dict, Any, Optional
import os
import time
import asyncio
import aiohttp
from concurrent.futures import ThreadPoolExecutor
import json

class ProperlyOptimizedRAG:
    """
    LLMì„ ìœ ì§€í•˜ë©´ì„œ ìµœì í™”í•˜ëŠ” ì˜¬ë°”ë¥¸ ë°©ë²•
    """

    def __init__(self, vector_store: FAISS):
        self.vector_store = vector_store

        # 1. ë” ë¹ ë¥¸ ëª¨ë¸ ì‚¬ìš© (gpt-3.5-turbo = ì•ˆì •ì ì¸ ë²„ì „)
        self.llm = ChatOpenAI(
            model="gpt-3.5-turbo",  # ì•ˆì •ì ì¸ ëª¨ë¸
            temperature=0.1,  # ì¼ê´€ì„±
            max_tokens=300,  # ë‹µë³€ ê¸¸ì´ ì œí•œìœ¼ë¡œ ì†ë„ í–¥ìƒ
            api_key=os.getenv("OPENAI_API_KEY"),
            request_timeout=5
        )

        # 2. ìµœì í™”ëœ ì§§ì€ í”„ë¡¬í”„íŠ¸
        self.prompt_template = self._create_minimal_prompt()

        # 3. ìŠ¤ë§ˆíŠ¸ ìºì‹± (LRU ìºì‹œ)
        self.cache = {}  # {question_hash: (answer, timestamp)}
        self.cache_ttl = 3600  # 1ì‹œê°„

        # 4. ë¹„ë™ê¸° ì²˜ë¦¬ ì¤€ë¹„
        self.executor = ThreadPoolExecutor(max_workers=2)

    def _create_minimal_prompt(self) -> PromptTemplate:
        """
        ìµœì†Œí•œì˜ í”„ë¡¬í”„íŠ¸ (í† í° ìˆ˜ ì¤„ì´ê¸°)
        """
        # ì§§ê³  ëª…í™•í•œ í”„ë¡¬í”„íŠ¸ = ë¹ ë¥¸ ì²˜ë¦¬
        template = """ë§¤ë‰´ì–¼: {context}

ì§ˆë¬¸: {question}

ë§¤ë‰´ì–¼ ë‚´ìš©ë§Œìœ¼ë¡œ ê°„ë‹¨ëª…ë£Œí•˜ê²Œ ë‹µë³€:"""

        return PromptTemplate(
            template=template,
            input_variables=["context", "question"]
        )

    def answer_question(self, question: str) -> Dict[str, Any]:
        """
        ìµœì í™”ëœ ë‹µë³€ ìƒì„± (ì—¬ì „íˆ LLM ì‚¬ìš©)
        """
        start_time = time.time()

        # 1. ìºì‹œ í™•ì¸ (5ms)
        cache_key = hash(question)
        if cache_key in self.cache:
            cached_answer, cached_time = self.cache[cache_key]
            if time.time() - cached_time < self.cache_ttl:
                return {
                    "question": question,
                    "answer": cached_answer['answer'],
                    "source_pages": cached_answer['pages'],
                    "response_time": 0.01,  # ìºì‹œëŠ” 10ms
                    "cached": True
                }

        # 2. ë³‘ë ¬ ì²˜ë¦¬: ë²¡í„° ê²€ìƒ‰ê³¼ ì „ì²˜ë¦¬ ë™ì‹œ ì‹¤í–‰
        with self.executor as executor:
            # ë²¡í„° ê²€ìƒ‰ (ë¹„ë™ê¸°)
            future_search = executor.submit(
                self._fast_vector_search,
                question
            )

            # ê²€ìƒ‰ ê²°ê³¼ ëŒ€ê¸°
            search_results = future_search.result(timeout=0.5)

            if not search_results:
                return self._fallback_response(question, start_time)

            # 3. ì»¨í…ìŠ¤íŠ¸ ìµœì í™” (ì¤‘ë³µ ì œê±°, ìš”ì•½)
            context = self._optimize_context(search_results)

            # 4. LLM í˜¸ì¶œ (ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì²« í† í° ë¹ ë¥´ê²Œ)
            answer = self._fast_llm_call(question, context)

            # 5. í˜ì´ì§€ ì •ë³´ ì¶”ì¶œ
            pages = list(set([
                doc.metadata.get('page', 0)
                for doc in search_results
            ]))[:3]

            response = {
                "question": question,
                "answer": answer,
                "source_pages": sorted(pages),
                "response_time": time.time() - start_time,
                "cached": False
            }

            # 6. ìºì‹œ ì €ì¥
            self.cache[cache_key] = (
                {"answer": answer, "pages": pages},
                time.time()
            )

            return response

    def _fast_vector_search(self, question: str, k: int = 3) -> List[Document]:
        """
        ë¹ ë¥¸ ë²¡í„° ê²€ìƒ‰ (kë¥¼ ì¤„ì´ê³  MMR ì œê±°)
        """
        try:
            # similarity_searchê°€ similarity_search_with_scoreë³´ë‹¤ ë¹ ë¦„
            docs = self.vector_store.similarity_search(
                question,
                k=k,  # 3ê°œë§Œ ê²€ìƒ‰
                fetch_k=k  # MMR ë¹„í™œì„±í™”
            )
            return docs
        except Exception as e:
            print(f"ë²¡í„° ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []

    def _optimize_context(self, docs: List[Document]) -> str:
        """
        ì»¨í…ìŠ¤íŠ¸ ìµœì í™” (ì¤‘ë³µ ì œê±°, í•µì‹¬ë§Œ ì¶”ì¶œ)
        """
        seen_content = set()
        optimized = []

        for doc in docs:
            content = doc.page_content.strip()

            # ì¤‘ë³µ ì²´í¬
            content_hash = hash(content[:50])  # ì• 50ìë¡œ ì¤‘ë³µ ì²´í¬
            if content_hash in seen_content:
                continue
            seen_content.add(content_hash)

            # ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸´ ë‚´ìš© ì œì™¸
            if len(content) < 50 or len(content) > 500:
                content = content[:500]  # ìµœëŒ€ 500ì

            optimized.append(content)

        # ìµœëŒ€ 3ê°œ ì²­í¬ë§Œ ì‚¬ìš© (í† í° ìˆ˜ ì œí•œ)
        return "\n---\n".join(optimized[:3])

    def _fast_llm_call(self, question: str, context: str) -> str:
        """
        ë¹ ë¥¸ LLM í˜¸ì¶œ
        """
        try:
            # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            prompt = self.prompt_template.format(
                context=context,
                question=question
            )

            # LLM í˜¸ì¶œ (invokeê°€ ê°€ì¥ ë¹ ë¦„)
            response = self.llm.invoke(prompt)

            # ì‘ë‹µ ì¶”ì¶œ
            if hasattr(response, 'content'):
                return response.content
            else:
                return str(response)

        except Exception as e:
            print(f"LLM í˜¸ì¶œ ì˜¤ë¥˜: {e}")
            return "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    def _fallback_response(self, question: str, start_time: float) -> Dict:
        """
        í´ë°± ì‘ë‹µ (ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ)
        """
        return {
            "question": question,
            "answer": "í•´ë‹¹ ì •ë³´ë¥¼ ë§¤ë‰´ì–¼ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‘œí˜„ìœ¼ë¡œ ì§ˆë¬¸í•´ ì£¼ì‹œê±°ë‚˜, ì„œë¹„ìŠ¤ì„¼í„°(1577-0001)ë¡œ ë¬¸ì˜í•´ ì£¼ì„¸ìš”.",
            "source_pages": [],
            "response_time": time.time() - start_time,
            "cached": False
        }

    def batch_test(self, questions: List[str]) -> Dict[str, Any]:
        """
        ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ë° ì„±ëŠ¥ ì¸¡ì •
        """
        results = []
        times = []

        print("\n" + "="*60)
        print("ğŸš€ ìµœì í™”ëœ RAG ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ (LLM ìœ ì§€)")
        print("="*60)

        for i, question in enumerate(questions, 1):
            result = self.answer_question(question)
            results.append(result)
            times.append(result['response_time'])

            status = "ğŸ“¦ ìºì‹œ" if result.get('cached') else "ğŸ” ê²€ìƒ‰"
            print(f"\n[{i}] {question}")
            print(f"â±ï¸  {result['response_time']:.2f}ì´ˆ ({status})")
            print(f"ğŸ“„ í˜ì´ì§€: {result['source_pages']}")
            print(f"ğŸ’¬ {result['answer'][:150]}...")

        # í†µê³„
        avg_time = sum(times) / len(times)
        cached_count = sum(1 for r in results if r.get('cached'))

        print("\n" + "="*60)
        print(f"ğŸ“Š ì„±ëŠ¥ í†µê³„:")
        print(f"  í‰ê·  ì‘ë‹µ: {avg_time:.2f}ì´ˆ")
        print(f"  ìµœì†Œ/ìµœëŒ€: {min(times):.2f}ì´ˆ / {max(times):.2f}ì´ˆ")
        print(f"  ìºì‹œ ì ì¤‘: {cached_count}/{len(questions)}")
        print(f"  ëª©í‘œ ë‹¬ì„±: {'âœ…' if avg_time < 1.0 else 'âš ï¸ 1ì´ˆ ëª©í‘œ ë¯¸ë‹¬'}")
        print("="*60)

        return {
            "average": avg_time,
            "min": min(times),
            "max": max(times),
            "results": results
        }


# ì¶”ê°€: ë¹„ë™ê¸° ë²„ì „ (ë” ë¹ ë¦„)
class AsyncOptimizedRAG:
    """
    ë¹„ë™ê¸° ì²˜ë¦¬ë¡œ ë” ë¹ ë¥¸ ì‘ë‹µ (ì‹¤í—˜ì )
    """

    def __init__(self, vector_store: FAISS):
        self.vector_store = vector_store
        self.llm = ChatOpenAI(
            model="gpt-3.5-turbo-1106",
            temperature=0.1,
            max_tokens=300,
            api_key=os.getenv("OPENAI_API_KEY")
        )
        self.cache = {}

    async def answer_question_async(self, question: str) -> Dict[str, Any]:
        """
        ë¹„ë™ê¸° ë‹µë³€ ìƒì„±
        """
        start_time = time.time()

        # ë¹„ë™ê¸°ë¡œ ë²¡í„° ê²€ìƒ‰ê³¼ LLM ì¤€ë¹„ ë™ì‹œ ì‹¤í–‰
        search_task = asyncio.create_task(
            self._async_vector_search(question)
        )

        # ê²€ìƒ‰ ì™„ë£Œ ëŒ€ê¸°
        docs = await search_task

        if not docs:
            return {
                "question": question,
                "answer": "ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "source_pages": [],
                "response_time": time.time() - start_time
            }

        # LLM í˜¸ì¶œ
        context = "\n".join([d.page_content[:300] for d in docs[:3]])
        answer = await self._async_llm_call(question, context)

        return {
            "question": question,
            "answer": answer,
            "source_pages": [d.metadata.get('page', 0) for d in docs],
            "response_time": time.time() - start_time
        }

    async def _async_vector_search(self, question: str) -> List[Document]:
        """ë¹„ë™ê¸° ë²¡í„° ê²€ìƒ‰"""
        return await asyncio.to_thread(
            self.vector_store.similarity_search,
            question,
            k=3
        )

    async def _async_llm_call(self, question: str, context: str) -> str:
        """ë¹„ë™ê¸° LLM í˜¸ì¶œ"""
        prompt = f"ë§¤ë‰´ì–¼: {context}\nì§ˆë¬¸: {question}\në‹µë³€:"

        response = await asyncio.to_thread(
            self.llm.invoke,
            prompt
        )

        return response.content if hasattr(response, 'content') else str(response)


# ë©”ì¸ í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    from embeddings import VehicleManualEmbeddings
    import os

    # API í‚¤ í™•ì¸
    if not os.getenv("OPENAI_API_KEY"):
        print("OpenAI API Keyë¥¼ ì…ë ¥í•˜ì„¸ìš”:")
        api_key = input("sk-... : ").strip()
        os.environ["OPENAI_API_KEY"] = api_key

    # ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ
    print("ë²¡í„° ì¸ë±ìŠ¤ ë¡œë”© ì¤‘...")
    embedder = VehicleManualEmbeddings()
    vector_store = embedder.load_index()

    # ìµœì í™”ëœ RAG ì‹œìŠ¤í…œ
    rag = ProperlyOptimizedRAG(vector_store)

    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸
    test_questions = [
        "ì—”ì§„ì˜¤ì¼ êµì²´ ì£¼ê¸°ëŠ”?",
        "íƒ€ì´ì–´ ê³µê¸°ì••ì€ ì–¼ë§ˆ?",
        "ê²½ê³ ë“±ì´ ì¼œì¡Œì„ ë•Œ ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?",
        "ëˆˆê¸¸ ì£¼í–‰ ì‹œ ì£¼ì˜ì‚¬í•­",
        "ìš´ì „ì ë³´ì¡° ì‹œìŠ¤í…œ ì„¤ì •"
    ]

    # ì²« ë²ˆì§¸ ì‹¤í–‰ (ì½œë“œ ìŠ¤íƒ€íŠ¸)
    print("\n### 1ì°¨ ì‹¤í–‰ (ìºì‹œ ì—†ìŒ) ###")
    stats1 = rag.batch_test(test_questions)

    # ë‘ ë²ˆì§¸ ì‹¤í–‰ (ìºì‹œ í™œìš©)
    print("\n### 2ì°¨ ì‹¤í–‰ (ìºì‹œ í™œìš©) ###")
    stats2 = rag.batch_test(test_questions[:3])

    # ê°œì„ ìœ¨ ê³„ì‚°
    improvement = ((stats1['average'] - stats2['average']) / stats1['average']) * 100
    print(f"\nğŸ¯ ìºì‹œ íš¨ê³¼: {improvement:.1f}% ì†ë„ í–¥ìƒ")