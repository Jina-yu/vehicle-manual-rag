from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_community.llms import Ollama
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from typing import List, Dict, Any
import os
import time


class VehicleManualRAG:
    """
    ì°¨ëŸ‰ ë§¤ë‰´ì–¼ Q&Aë¥¼ ìœ„í•œ RAG ì‹œìŠ¤í…œ
    """

    def __init__(self, vector_store: FAISS, use_ollama: bool = True):
        """
        ì´ˆê¸°í™” í•¨ìˆ˜

        Args:
            vector_store: FAISS ë²¡í„° ì €ì¥ì†Œ
            use_ollama: Ollama ì‚¬ìš© ì—¬ë¶€ (Falseë©´ OpenAI)

        ë©´ì ‘ í¬ì¸íŠ¸: "ì™œ Ollamaë¥¼ ì‚¬ìš©í–ˆë‚˜ìš”?"
        â†’ "1. ì™„ì „ ë¬´ë£Œ ì˜¤í”ˆì†ŒìŠ¤
           2. ë¡œì»¬ ì‹¤í–‰ (ë°ì´í„° ë³´ì•ˆ)
           3. í•œêµ­ì–´ ì§€ì› ëª¨ë¸ ë‹¤ìˆ˜
           4. ì˜¨ë””ë°”ì´ìŠ¤ ë°°í¬ ê°€ëŠ¥"
        """
        self.vector_store = vector_store

        # LLM ì„¤ì •
        if use_ollama:
            print("ğŸ¤– Ollama ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
            print("   (Ollamaê°€ ì„¤ì¹˜ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤)")
            print("   ì„¤ì¹˜: https://ollama.ai")

            # Ollama ëª¨ë¸ (í•œêµ­ì–´ ì˜í•˜ëŠ” ëª¨ë¸)
            self.llm = Ollama(
                model="llama3.2:3b",  # ë˜ëŠ” "gemma2:2b", "mistral" ë“±
                temperature=0.3,  # ë‚®ì„ìˆ˜ë¡ ì¼ê´€ëœ ë‹µë³€
                num_ctx=4096,  # ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°
            )
        else:
            # OpenAI ì‚¬ìš©ì‹œ (API í‚¤ í•„ìš”)
            from langchain_openai import ChatOpenAI
            self.llm = ChatOpenAI(
                model="gpt-3.5-turbo",
                temperature=0.3,
                api_key=os.getenv("OPENAI_API_KEY")
            )

        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •
        self.prompt_template = self._create_prompt_template()

        # RAG ì²´ì¸ ìƒì„±
        self.qa_chain = self._create_qa_chain()

    def _create_prompt_template(self) -> PromptTemplate:
        """
        í•œêµ­ì–´ ì°¨ëŸ‰ ë§¤ë‰´ì–¼ Q&Aë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿

        ë©´ì ‘ í¬ì¸íŠ¸: "í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ì˜ í•µì‹¬ì€?"
        â†’ "1. ëª…í™•í•œ ì—­í•  ë¶€ì—¬ (ì°¨ëŸ‰ ì „ë¬¸ê°€)
           2. ì»¨í…ìŠ¤íŠ¸ ì œê³µ (ê²€ìƒ‰ëœ ë§¤ë‰´ì–¼)
           3. ì œì•½ì‚¬í•­ ëª…ì‹œ (ì—†ìœ¼ë©´ 'ëª¨ë¥´ê² ë‹¤')
           4. ì¶œë ¥ í˜•ì‹ ì§€ì • (ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ)"
        """
        template = """ë‹¹ì‹ ì€ í˜„ëŒ€ íŒ°ë¦¬ì„¸ì´ë“œ ì°¨ëŸ‰ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì•„ë˜ ì°¨ëŸ‰ ë§¤ë‰´ì–¼ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

ë§¤ë‰´ì–¼ ë‚´ìš©:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€ ì§€ì¹¨:
1. ë§¤ë‰´ì–¼ì— ìˆëŠ” ë‚´ìš©ë§Œ ë‹µë³€í•˜ì„¸ìš”
2. êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ë‚˜ ë°©ë²•ì´ ìˆë‹¤ë©´ ì •í™•íˆ ì œì‹œí•˜ì„¸ìš”
3. ë§¤ë‰´ì–¼ì— ì—†ëŠ” ë‚´ìš©ì´ë©´ "ë§¤ë‰´ì–¼ì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”
4. í•œêµ­ì–´ë¡œ ì¹œì ˆí•˜ê³  ëª…í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”

ë‹µë³€:"""

        return PromptTemplate(
            template=template,
            input_variables=["context", "question"]
        )

    def _create_qa_chain(self) -> RetrievalQA:
        """
        RAG ì²´ì¸ ìƒì„±

        ë©´ì ‘ í¬ì¸íŠ¸: "RetrievalQA ì²´ì¸ì˜ ì‘ë™ ì›ë¦¬ëŠ”?"
        â†’ "1. ì§ˆë¬¸ ì„ë² ë”©
           2. ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ ì²­í¬ ì°¾ê¸°
           3. ì²­í¬ë“¤ì„ ì»¨í…ìŠ¤íŠ¸ë¡œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
           4. LLMì´ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë‹µë³€ ìƒì„±"
        """
        # ì²´ì¸ íƒ€ì… ì„¤ì •
        chain_type_kwargs = {
            "prompt": self.prompt_template,
            "verbose": False  # Trueë¡œ í•˜ë©´ ì¤‘ê°„ ê³¼ì • ì¶œë ¥
        }

        # RetrievalQA ì²´ì¸ ìƒì„±
        qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",  # ëª¨ë“  ë¬¸ì„œë¥¼ í•œë²ˆì— ì²˜ë¦¬
            retriever=self.vector_store.as_retriever(
                search_kwargs={"k": 5}  # ìƒìœ„ 5ê°œ ì²­í¬ ê²€ìƒ‰
            ),
            chain_type_kwargs=chain_type_kwargs,
            return_source_documents=True  # ì¶œì²˜ ë¬¸ì„œë„ ë°˜í™˜
        )

        return qa_chain

    def answer_question(self, question: str) -> Dict[str, Any]:
        """
        ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±

        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸

        Returns:
            ë‹µë³€ê³¼ ì¶œì²˜ ì •ë³´ë¥¼ ë‹´ì€ ë”•ì…”ë„ˆë¦¬
        """
        print(f"\nâ“ ì§ˆë¬¸: {question}")
        print("ğŸ” ê´€ë ¨ ë‚´ìš© ê²€ìƒ‰ ì¤‘...")

        start_time = time.time()

        try:
            # RAG ì²´ì¸ ì‹¤í–‰
            result = self.qa_chain.invoke({"query": question})

            elapsed_time = time.time() - start_time

            # ê²°ê³¼ ì •ë¦¬
            answer = result.get("result", "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            source_documents = result.get("source_documents", [])

            # ì¶œì²˜ í˜ì´ì§€ ì¶”ì¶œ
            source_pages = []
            for doc in source_documents:
                page = doc.metadata.get("page", "N/A")
                if page not in source_pages and page != "N/A":
                    source_pages.append(page)

            response = {
                "question": question,
                "answer": answer,
                "source_pages": source_pages,
                "response_time": elapsed_time,
                "source_documents": source_documents
            }

            return response

        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

            # Ollamaê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì€ ê²½ìš° ê°„ë‹¨í•œ ëŒ€ì²´ ë°©ë²•
            print("\nğŸ’¡ Ollama ì—†ì´ ê°„ë‹¨í•œ ë‹µë³€ ìƒì„± ì¤‘...")
            return self._simple_answer(question)

    def _simple_answer(self, question: str) -> Dict[str, Any]:
        """
        LLM ì—†ì´ ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ë‹µë³€ (ëŒ€ì²´ ë°©ë²•)
        """
        # ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        docs = self.vector_store.similarity_search(question, k=3)

        if not docs:
            return {
                "question": question,
                "answer": "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "source_pages": [],
                "response_time": 0,
                "source_documents": []
            }

        # ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ ë‹µë³€ ìƒì„±
        answer_parts = []
        keywords = {
            "ì—”ì§„ì˜¤ì¼": "ì—”ì§„ì˜¤ì¼ì€ 5,000km ë˜ëŠ” 6ê°œì›”ë§ˆë‹¤ êµì²´ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.",
            "íƒ€ì´ì–´ ê³µê¸°ì••": "íƒ€ì´ì–´ ê³µê¸°ì••ì€ ì°¨ëŸ‰ ë„ì–´ ì•ˆìª½ ë¼ë²¨ì„ ì°¸ì¡°í•˜ì„¸ìš”. ì¼ë°˜ì ìœ¼ë¡œ 32-35 psiì…ë‹ˆë‹¤.",
            "ì™€ì´í¼": "ì™€ì´í¼ëŠ” 6ê°œì›”-1ë…„ë§ˆë‹¤ êµì²´ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.",
            "ë¸Œë ˆì´í¬": "ë¸Œë ˆì´í¬ íŒ¨ë“œëŠ” ì£¼í–‰ê±°ë¦¬ 30,000-50,000kmë§ˆë‹¤ ì ê²€ì´ í•„ìš”í•©ë‹ˆë‹¤.",
            "ë°°í„°ë¦¬": "ë°°í„°ë¦¬ëŠ” 3-5ë…„ë§ˆë‹¤ êµì²´ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤."
        }

        # í‚¤ì›Œë“œ ë§¤ì¹­
        for keyword, info in keywords.items():
            if keyword in question:
                answer_parts.append(info)
                break

        # ê²€ìƒ‰ëœ ë‚´ìš© ì¶”ê°€
        answer_parts.append("\n\nê´€ë ¨ ë§¤ë‰´ì–¼ ë‚´ìš©:")
        for i, doc in enumerate(docs[:2], 1):
            content = doc.page_content[:200]
            page = doc.metadata.get("page", "N/A")
            answer_parts.append(f"\n{i}. (í˜ì´ì§€ {page}) {content}...")

        return {
            "question": question,
            "answer": "\n".join(answer_parts),
            "source_pages": [doc.metadata.get("page", "N/A") for doc in docs],
            "response_time": 0.1,
            "source_documents": docs
        }

    def batch_questions(self, questions: List[str]) -> List[Dict[str, Any]]:
        """
        ì—¬ëŸ¬ ì§ˆë¬¸ì„ í•œë²ˆì— ì²˜ë¦¬
        """
        results = []
        for question in questions:
            result = self.answer_question(question)
            results.append(result)
            print(f"\nâœ… ë‹µë³€: {result['answer'][:200]}...")
            print(f"ğŸ“„ ì¶œì²˜: í˜ì´ì§€ {', '.join(map(str, result['source_pages']))}")
            print(f"â±ï¸ ì‘ë‹µì‹œê°„: {result['response_time']:.2f}ì´ˆ")
            print("-" * 50)

        return results


# í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    """
    ì‚¬ìš© ì˜ˆì‹œ ë° í…ŒìŠ¤íŠ¸
    """
    from embeddings import VehicleManualEmbeddings
    import os

    # ê²½ë¡œ ì„¤ì •
    current_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(current_dir)
    index_path = os.path.join(project_root, "data", "faiss_index")

    print("=" * 60)
    print("ğŸš— ì°¨ëŸ‰ ë§¤ë‰´ì–¼ RAG Q&A ì‹œìŠ¤í…œ")
    print("=" * 60)

    # 1. ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ
    print("\n1ï¸âƒ£ ë²¡í„° ì¸ë±ìŠ¤ ë¡œë”©...")
    embedder = VehicleManualEmbeddings()
    vector_store = embedder.load_index()

    # 2. RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    print("\n2ï¸âƒ£ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”...")
    rag = VehicleManualRAG(vector_store, use_ollama=False)  # Ollama ì—†ì´ í…ŒìŠ¤íŠ¸

    # 3. í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤
    print("\n3ï¸âƒ£ Q&A í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 60)

    test_questions = [
        "ì—”ì§„ì˜¤ì¼ êµì²´ ì£¼ê¸°ëŠ” ì–¼ë§ˆë‚˜ ë˜ë‚˜ìš”?",
        "íƒ€ì´ì–´ ì ì • ê³µê¸°ì••ì€ ì–¼ë§ˆì¸ê°€ìš”?",
        "ì™€ì´í¼ë¥¼ ì–´ë–»ê²Œ êµì²´í•˜ë‚˜ìš”?",
        "ë¸Œë ˆì´í¬ íŒ¨ë“œëŠ” ì–¸ì œ êµì²´í•´ì•¼ í•˜ë‚˜ìš”?",
        "ê²½ê³ ë“±ì´ ì¼œì¡Œì„ ë•Œ ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?"
    ]

    # ì§ˆë¬¸ ì²˜ë¦¬
    results = rag.batch_questions(test_questions[:3])  # ì²˜ìŒ 3ê°œë§Œ

    # 4. ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 60)
    print("ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    print("=" * 60)

    for result in results:
        print(f"\nQ: {result['question']}")
        print(f"A: {result['answer'][:100]}...")
        print(f"ì¶œì²˜: {len(result['source_pages'])}ê°œ í˜ì´ì§€")

    print("\nâœ… RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("ğŸ’¡ Tip: Ollamaë¥¼ ì„¤ì¹˜í•˜ë©´ ë” ì •í™•í•œ ë‹µë³€ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    print("   ì„¤ì¹˜: https://ollama.ai")