import time
import json
import os
import sys
import numpy as np
from typing import List, Dict, Tuple, Any
from dataclasses import dataclass
from datetime import datetime
import re
from collections import Counter


sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))
from dotenv import load_dotenv

load_dotenv()

from src.embeddings import VehicleManualEmbeddings
from src.rag_chain import VehicleManualRAG


from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import warnings

warnings.filterwarnings('ignore')


@dataclass
class EvaluationMetrics:

    semantic_similarity: float  # ì˜ë¯¸ ìœ ì‚¬ë„
    answer_relevance: float  # ë‹µë³€ ê´€ë ¨ì„±
    faithfulness: float  # ì›ë¬¸ ì¶©ì‹¤ë„
    completeness: float  # ë‹µë³€ ì™„ì „ì„±
    response_time: float  # ì‘ë‹µ ì‹œê°„
    consistency: float  # ì¼ê´€ì„± ì ìˆ˜

    @property
    def overall_score(self) -> float:

        weights = {
            'semantic_similarity': 0.20,
            'answer_relevance': 0.25,
            'faithfulness': 0.25,
            'completeness': 0.15,
            'consistency': 0.15
        }

        score = (
                self.semantic_similarity * weights['semantic_similarity'] +
                self.answer_relevance * weights['answer_relevance'] +
                self.faithfulness * weights['faithfulness'] +
                self.completeness * weights['completeness'] +
                self.consistency * weights['consistency']
        )
        return score


class RAGEvaluator:
    """
    RAG ì‹œìŠ¤í…œ ì¢…í•© í‰ê°€ í´ë˜ìŠ¤

    í‰ê°€ ì§€í‘œ ì„ ì • ì´ìœ :
    1. Semantic Similarity: ë‹µë³€ì´ ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ ì–¼ë§ˆë‚˜ ì˜ ì´í•´í–ˆëŠ”ì§€ ì¸¡ì •
    2. Answer Relevance: ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ì–¼ë§ˆë‚˜ ê´€ë ¨ìˆëŠ”ì§€ ì¸¡ì •
    3. Faithfulness: ë‹µë³€ì´ ì†ŒìŠ¤ ë¬¸ì„œë¥¼ ì–¼ë§ˆë‚˜ ì •í™•íˆ ë°˜ì˜í•˜ëŠ”ì§€ ì¸¡ì •
    4. Completeness: ë‹µë³€ì´ í•„ìš”í•œ ì •ë³´ë¥¼ ëª¨ë‘ í¬í•¨í•˜ëŠ”ì§€ ì¸¡ì •
    5. Consistency: ê°™ì€ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì˜ ì¼ê´€ì„± ì¸¡ì •
    6. Response Time: ì‹¤ì‹œê°„ ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ ì¶©ì¡± ì—¬ë¶€ ì¸¡ì •
    """

    def __init__(self):
        """í‰ê°€ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        print("ğŸ”§ í‰ê°€ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")

        # ì˜ë¯¸ ìœ ì‚¬ë„ ì¸¡ì •ì„ ìœ„í•œ ì„ë² ë”© ëª¨ë¸
        self.embed_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')

        # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        embedder = VehicleManualEmbeddings()
        vector_store = embedder.load_index()
        self.rag_system = VehicleManualRAG(vector_store, use_ollama=False)

        print("âœ… í‰ê°€ ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ\n")

    def evaluate_semantic_similarity(self, question: str, answer: str) -> float:
        """
        1. ì˜ë¯¸ ìœ ì‚¬ë„ í‰ê°€ (Semantic Similarity Score)

        ì´ìœ : ë‹µë³€ì´ ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ ì œëŒ€ë¡œ ì´í•´í–ˆëŠ”ì§€ ì¸¡ì •
        ë°©ë²•: ì§ˆë¬¸ê³¼ ë‹µë³€ì˜ ì„ë² ë”© ë²¡í„° ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°

        Returns:
            0~1 ì‚¬ì´ì˜ ìœ ì‚¬ë„ ì ìˆ˜
        """
        # ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ì„ë² ë”©
        q_embedding = self.embed_model.encode([question])
        a_embedding = self.embed_model.encode([answer])

        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarity = cosine_similarity(q_embedding, a_embedding)[0][0]

        # ìŠ¤ì¼€ì¼ ì¡°ì • (0.3~0.8 â†’ 0~1)
        scaled_similarity = max(0, min(1, (similarity - 0.3) / 0.5))

        return scaled_similarity

    def evaluate_answer_relevance(self, question: str, answer: str) -> float:
        """
        2. ë‹µë³€ ê´€ë ¨ì„± í‰ê°€ (Answer Relevance Score)

        ì´ìœ : ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ìˆëŠ” ë‚´ìš©ì„ ë‹´ê³  ìˆëŠ”ì§€ ì¸¡ì •
        ë°©ë²•: í‚¤ì›Œë“œ ë§¤ì¹­ + ì˜ë¯¸ì  ê´€ë ¨ì„± ì²´í¬

        Returns:
            0~1 ì‚¬ì´ì˜ ê´€ë ¨ì„± ì ìˆ˜
        """
        # ì§ˆë¬¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
        question_keywords = self._extract_keywords(question)
        answer_lower = answer.lower()

        # í‚¤ì›Œë“œ ë§¤ì¹­ë¥  ê³„ì‚°
        matched = sum(1 for kw in question_keywords if kw in answer_lower)
        keyword_score = matched / len(question_keywords) if question_keywords else 0

        # ë‹µë³€ì´ "ëª¨ë¥´ê² ë‹¤", "ì°¾ì„ ìˆ˜ ì—†ë‹¤" ë“± ë¶€ì •ì  ì‘ë‹µì¸ì§€ ì²´í¬
        negative_responses = ['ì°¾ì„ ìˆ˜ ì—†', 'ëª¨ë¥´ê² ', 'ì •ë³´ê°€ ì—†', 'ë§¤ë‰´ì–¼ì— ì—†']
        has_negative = any(neg in answer for neg in negative_responses)

        # ë¶€ì •ì  ì‘ë‹µì´ë©´ ê´€ë ¨ì„± ë‚®ìŒ
        if has_negative:
            return max(0.2, keyword_score * 0.5)

        # ë‹µë³€ ê¸¸ì´ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ê´€ë ¨ì„± ì˜ì‹¬
        if len(answer) < 20:
            return keyword_score * 0.7

        return min(1.0, keyword_score + 0.3)  # ê¸°ë³¸ ì ìˆ˜ 0.3 ë¶€ì—¬

    def evaluate_faithfulness(self, answer: str, source_docs: List[Any]) -> float:
        """
        3. ì›ë¬¸ ì¶©ì‹¤ë„ í‰ê°€ (Faithfulness Score)

        ì´ìœ : ë‹µë³€ì´ ì†ŒìŠ¤ ë¬¸ì„œì˜ ë‚´ìš©ì„ ì™œê³¡ ì—†ì´ ì •í™•íˆ ì „ë‹¬í•˜ëŠ”ì§€ ì¸¡ì •
        ë°©ë²•: ë‹µë³€ì˜ í•µì‹¬ ì •ë³´ê°€ ì†ŒìŠ¤ ë¬¸ì„œì— ì¡´ì¬í•˜ëŠ”ì§€ ê²€ì¦

        Returns:
            0~1 ì‚¬ì´ì˜ ì¶©ì‹¤ë„ ì ìˆ˜
        """
        if not source_docs:
            return 0.3  # ì†ŒìŠ¤ê°€ ì—†ìœ¼ë©´ ë‚®ì€ ì ìˆ˜

        # ì†ŒìŠ¤ ë¬¸ì„œë“¤ì„ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ ê²°í•©
        source_text = " ".join([doc.page_content for doc in source_docs]).lower()

        # ë‹µë³€ì—ì„œ ìˆ«ì, ë‹¨ìœ„, ê³ ìœ ëª…ì‚¬ ë“± íŒ©íŠ¸ ì¶”ì¶œ
        facts = self._extract_facts(answer)

        if not facts:
            # íŒ©íŠ¸ê°€ ì—†ìœ¼ë©´ ì¼ë°˜ì ì¸ í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ë¡œ í‰ê°€
            answer_embedding = self.embed_model.encode([answer])
            source_embedding = self.embed_model.encode([source_text[:1000]])  # ì²˜ìŒ 1000ìë§Œ
            similarity = cosine_similarity(answer_embedding, source_embedding)[0][0]
            return max(0.5, similarity)  # ìµœì†Œ 0.5ì 

        # íŒ©íŠ¸ê°€ ì†ŒìŠ¤ì— ìˆëŠ”ì§€ í™•ì¸
        found_facts = sum(1 for fact in facts if fact.lower() in source_text)
        faithfulness = found_facts / len(facts)

        return faithfulness

    def evaluate_completeness(self, question: str, answer: str) -> float:
        """
        4. ë‹µë³€ ì™„ì „ì„± í‰ê°€ (Completeness Score)

        ì´ìœ : ì‚¬ìš©ì ì§ˆë¬¸ì— í•„ìš”í•œ ëª¨ë“  ì •ë³´ê°€ ë‹µë³€ì— í¬í•¨ë˜ì—ˆëŠ”ì§€ ì¸¡ì •
        ë°©ë²•: ì§ˆë¬¸ ìœ í˜•ë³„ í•„ìˆ˜ ìš”ì†Œ ì²´í¬

        Returns:
            0~1 ì‚¬ì´ì˜ ì™„ì „ì„± ì ìˆ˜
        """
        score = 0.5  # ê¸°ë³¸ ì ìˆ˜

        # ì§ˆë¬¸ ìœ í˜• íŒŒì•…
        question_lower = question.lower()

        # "ì–´ë–»ê²Œ" ì§ˆë¬¸ â†’ ë‹¨ê³„ë³„ ì„¤ëª… í•„ìš”
        if any(word in question_lower for word in ['ì–´ë–»ê²Œ', 'ë°©ë²•', 'ì ˆì°¨']):
            # ìˆœì„œ í‘œì‹œìë‚˜ ë‹¨ê³„ êµ¬ë¶„ì´ ìˆëŠ”ì§€ ì²´í¬
            has_steps = any(marker in answer for marker in ['1.', '2.', 'ì²«ì§¸', 'ë‘˜ì§¸', 'ë¨¼ì €', 'ë‹¤ìŒ'])
            if has_steps:
                score += 0.3
            if len(answer) > 100:  # ì¶©ë¶„í•œ ì„¤ëª…
                score += 0.2

        # "ì–¸ì œ", "ì£¼ê¸°" ì§ˆë¬¸ â†’ êµ¬ì²´ì  ìˆ˜ì¹˜ í•„ìš”
        elif any(word in question_lower for word in ['ì–¸ì œ', 'ì£¼ê¸°', 'ì–¼ë§ˆë‚˜', 'ëª‡']):
            # ìˆ«ìì™€ ë‹¨ìœ„ê°€ ìˆëŠ”ì§€ ì²´í¬
            has_numbers = bool(re.search(r'\d+', answer))
            has_units = any(unit in answer for unit in ['km', 'ê°œì›”', 'ë…„', 'ì¼', 'psi', 'ì‹œê°„'])
            if has_numbers and has_units:
                score += 0.5
            elif has_numbers or has_units:
                score += 0.3

        # "ë¬´ì—‡", "ë­" ì§ˆë¬¸ â†’ ì •ì˜ë‚˜ ì„¤ëª… í•„ìš”
        elif any(word in question_lower for word in ['ë¬´ì—‡', 'ë­', 'ë­”ê°€ìš”']):
            if len(answer) > 50:  # ì¶©ë¶„í•œ ì„¤ëª…
                score += 0.3
            if 'ì…ë‹ˆë‹¤' in answer or 'ìˆìŠµë‹ˆë‹¤' in answer:  # ëª…í™•í•œ ì •ì˜
                score += 0.2

        # ì¼ë°˜ì ì¸ ì™„ì „ì„± ì²´í¬
        else:
            # ë‹µë³€ ê¸¸ì´
            if len(answer) > 100:
                score += 0.3
            elif len(answer) > 50:
                score += 0.2

            # êµ¬ì²´ì  ì •ë³´ í¬í•¨ ì—¬ë¶€
            if any(char.isdigit() for char in answer):
                score += 0.2

        return min(1.0, score)

    def evaluate_consistency(self, question: str, num_trials: int = 3) -> float:
        """
        5. ì¼ê´€ì„± í‰ê°€ (Consistency Score)

        ì´ìœ : ê°™ì€ ì§ˆë¬¸ì— ëŒ€í•´ ì¼ê´€ëœ ë‹µë³€ì„ ì œê³µí•˜ëŠ”ì§€ ì¸¡ì • (ì‹ ë¢°ì„±)
        ë°©ë²•: ê°™ì€ ì§ˆë¬¸ì„ ì—¬ëŸ¬ ë²ˆ í•˜ì—¬ ë‹µë³€ì˜ ìœ ì‚¬ë„ ì¸¡ì •

        Returns:
            0~1 ì‚¬ì´ì˜ ì¼ê´€ì„± ì ìˆ˜
        """
        if num_trials < 2:
            return 1.0

        answers = []
        for _ in range(num_trials):
            response = self.rag_system.answer_question(question)
            answers.append(response['answer'])
            time.sleep(0.1)  # API í˜¸ì¶œ ê°„ê²©

        # ëª¨ë“  ë‹µë³€ ìŒì˜ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = []
        for i in range(len(answers)):
            for j in range(i + 1, len(answers)):
                emb1 = self.embed_model.encode([answers[i]])
                emb2 = self.embed_model.encode([answers[j]])
                sim = cosine_similarity(emb1, emb2)[0][0]
                similarities.append(sim)

        # í‰ê·  ìœ ì‚¬ë„ê°€ ì¼ê´€ì„± ì ìˆ˜
        consistency = np.mean(similarities) if similarities else 1.0

        return consistency

    def _extract_keywords(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ë¶ˆìš©ì–´ ì œê±°
        stopwords = ['ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì˜', 'ì—', 'ì™€', 'ê³¼', 'ë„', 'ë¡œ', 'ìœ¼ë¡œ', 'ë§Œ', 'ê¹Œì§€']

        # ëª…ì‚¬ì™€ ì£¼ìš” ë‹¨ì–´ ì¶”ì¶œ (ê°„ë‹¨í•œ ë°©ë²•)
        words = re.findall(r'[ê°€-í£]+|[a-zA-Z]+|\d+', text.lower())
        keywords = [w for w in words if len(w) > 1 and w not in stopwords]

        return keywords[:5]  # ìƒìœ„ 5ê°œ í‚¤ì›Œë“œ

    def _extract_facts(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ íŒ©íŠ¸(ìˆ«ì, ë‹¨ìœ„, ê³ ìœ ëª…ì‚¬) ì¶”ì¶œ"""
        facts = []

        # ìˆ«ìì™€ ë‹¨ìœ„ ì¡°í•© (ì˜ˆ: 5000km, 35psi)
        number_units = re.findall(r'\d+[ê°€-í£a-zA-Z]+', text)
        facts.extend(number_units)

        # ë…ë¦½ì ì¸ ìˆ«ì
        numbers = re.findall(r'\d+', text)
        facts.extend(numbers)

        # ì˜ì–´ ì•½ì–´ (ëŒ€ë¬¸ìë¡œ ì‹œì‘)
        abbreviations = re.findall(r'\b[A-Z]{2,}\b', text)
        facts.extend(abbreviations)

        return facts

    def evaluate_single_qa(self, question: str, check_consistency: bool = False) -> Dict[str, Any]:
        """
        ë‹¨ì¼ ì§ˆë¬¸-ë‹µë³€ ìŒì— ëŒ€í•œ ì¢…í•© í‰ê°€
        """
        print(f"\nğŸ“ í‰ê°€ ì¤‘: {question}")
        print("-" * 50)

        # RAG ì‹œìŠ¤í…œìœ¼ë¡œ ë‹µë³€ ìƒì„±
        start_time = time.time()
        response = self.rag_system.answer_question(question)
        response_time = time.time() - start_time

        answer = response['answer']
        source_docs = response.get('source_documents', [])

        # ê° ì§€í‘œ í‰ê°€
        metrics = EvaluationMetrics(
            semantic_similarity=self.evaluate_semantic_similarity(question, answer),
            answer_relevance=self.evaluate_answer_relevance(question, answer),
            faithfulness=self.evaluate_faithfulness(answer, source_docs),
            completeness=self.evaluate_completeness(question, answer),
            response_time=response_time,
            consistency=self.evaluate_consistency(question, 2) if check_consistency else 1.0
        )

        # ê²°ê³¼ ì¶œë ¥
        print(f"  ğŸ“Š ì˜ë¯¸ ìœ ì‚¬ë„: {metrics.semantic_similarity:.2%}")
        print(f"  ğŸ“Š ë‹µë³€ ê´€ë ¨ì„±: {metrics.answer_relevance:.2%}")
        print(f"  ğŸ“Š ì›ë¬¸ ì¶©ì‹¤ë„: {metrics.faithfulness:.2%}")
        print(f"  ğŸ“Š ë‹µë³€ ì™„ì „ì„±: {metrics.completeness:.2%}")
        if check_consistency:
            print(f"  ğŸ“Š ì¼ê´€ì„±: {metrics.consistency:.2%}")
        print(f"  â±ï¸  ì‘ë‹µ ì‹œê°„: {metrics.response_time:.2f}ì´ˆ")
        print(f"  âœ¨ ì¢…í•© ì ìˆ˜: {metrics.overall_score:.2%}")

        return {
            'question': question,
            'answer': answer[:200] + '...' if len(answer) > 200 else answer,
            'metrics': metrics,
            'source_pages': response.get('source_pages', [])
        }

    def evaluate_test_set(self, test_cases: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ì „ì²´ í‰ê°€
        """
        print("\n" + "=" * 60)
        print("ğŸ”¬ RAG ì‹œìŠ¤í…œ ì¢…í•© í‰ê°€ ì‹œì‘")
        print("=" * 60)

        all_results = []

        for i, test_case in enumerate(test_cases, 1):
            print(f"\n[{i}/{len(test_cases)}]", end="")
            result = self.evaluate_single_qa(
                test_case['question'],
                check_consistency=test_case.get('check_consistency', False)
            )
            result['category'] = test_case.get('category', 'general')
            result['expected'] = test_case.get('expected', '')
            all_results.append(result)

        # ì „ì²´ í†µê³„ ê³„ì‚°
        avg_metrics = self._calculate_average_metrics(all_results)
        category_analysis = self._analyze_by_category(all_results)

        # ì¢…í•© ë³´ê³ ì„œ
        report = {
            'timestamp': datetime.now().isoformat(),
            'total_questions': len(test_cases),
            'average_metrics': avg_metrics,
            'category_analysis': category_analysis,
            'detailed_results': all_results,
            'system_assessment': self._generate_assessment(avg_metrics)
        }

        return report

    def _calculate_average_metrics(self, results: List[Dict]) -> Dict[str, float]:
        """í‰ê·  ì§€í‘œ ê³„ì‚°"""
        metrics_sum = {
            'semantic_similarity': 0,
            'answer_relevance': 0,
            'faithfulness': 0,
            'completeness': 0,
            'consistency': 0,
            'response_time': 0,
            'overall_score': 0
        }

        for result in results:
            m = result['metrics']
            metrics_sum['semantic_similarity'] += m.semantic_similarity
            metrics_sum['answer_relevance'] += m.answer_relevance
            metrics_sum['faithfulness'] += m.faithfulness
            metrics_sum['completeness'] += m.completeness
            metrics_sum['consistency'] += m.consistency
            metrics_sum['response_time'] += m.response_time
            metrics_sum['overall_score'] += m.overall_score

        n = len(results)
        return {k: v / n for k, v in metrics_sum.items()}

    def _analyze_by_category(self, results: List[Dict]) -> Dict[str, Dict]:
        """ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„"""
        categories = {}

        for result in results:
            cat = result['category']
            if cat not in categories:
                categories[cat] = []
            categories[cat].append(result['metrics'])

        category_stats = {}
        for cat, metrics_list in categories.items():
            category_stats[cat] = {
                'count': len(metrics_list),
                'avg_overall': np.mean([m.overall_score for m in metrics_list]),
                'avg_response_time': np.mean([m.response_time for m in metrics_list])
            }

        return category_stats

    def _generate_assessment(self, avg_metrics: Dict[str, float]) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ í‰ê°€ ë° ê°œì„  ì œì•ˆ ìƒì„±"""
        assessment = {
            'overall_grade': '',
            'strengths': [],
            'weaknesses': [],
            'recommendations': []
        }

        # ì¢…í•© ë“±ê¸‰ ë¶€ì—¬
        overall = avg_metrics['overall_score']
        if overall >= 0.9:
            assessment['overall_grade'] = 'A+ (íƒì›”í•¨)'
        elif overall >= 0.8:
            assessment['overall_grade'] = 'A (ìš°ìˆ˜í•¨)'
        elif overall >= 0.7:
            assessment['overall_grade'] = 'B (ì–‘í˜¸í•¨)'
        elif overall >= 0.6:
            assessment['overall_grade'] = 'C (ê°œì„  í•„ìš”)'
        else:
            assessment['overall_grade'] = 'D (ì‹¬ê°í•œ ê°œì„  í•„ìš”)'

        # ê°•ì  ë¶„ì„
        if avg_metrics['semantic_similarity'] >= 0.8:
            assessment['strengths'].append('ì§ˆë¬¸ ì˜ë„ íŒŒì•… ìš°ìˆ˜')
        if avg_metrics['faithfulness'] >= 0.8:
            assessment['strengths'].append('ì›ë¬¸ ê¸°ë°˜ ì •í™•í•œ ë‹µë³€')
        if avg_metrics['response_time'] <= 1.0:
            assessment['strengths'].append('ë¹ ë¥¸ ì‘ë‹µ ì†ë„')

        # ì•½ì  ë¶„ì„
        if avg_metrics['answer_relevance'] < 0.7:
            assessment['weaknesses'].append('ë‹µë³€ ê´€ë ¨ì„± ë¶€ì¡±')
            assessment['recommendations'].append('í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ê°œì„  í•„ìš”')

        if avg_metrics['completeness'] < 0.7:
            assessment['weaknesses'].append('ë¶ˆì™„ì „í•œ ë‹µë³€')
            assessment['recommendations'].append('ê²€ìƒ‰ ì²­í¬ ìˆ˜ ì¦ê°€ ê³ ë ¤')

        if avg_metrics['consistency'] < 0.8:
            assessment['weaknesses'].append('ì¼ê´€ì„± ë¶€ì¡±')
            assessment['recommendations'].append('Temperature íŒŒë¼ë¯¸í„° ë‚®ì¶”ê¸° (í˜„ì¬ 0.3 â†’ 0.1)')

        if avg_metrics['response_time'] > 2.0:
            assessment['weaknesses'].append('ëŠë¦° ì‘ë‹µ ì†ë„')
            assessment['recommendations'].append('ìºì‹± ì „ëµ ë„ì… ë˜ëŠ” ëª¨ë¸ ê²½ëŸ‰í™”')

        return assessment

    def save_evaluation_report(self, report: Dict[str, Any], filename: str = 'evaluation_report.json'):
        """í‰ê°€ ë³´ê³ ì„œ ì €ì¥"""
        # JSON ì§ë ¬í™”ë¥¼ ìœ„í•´ metrics ê°ì²´ë¥¼ dictë¡œ ë³€í™˜
        serializable_report = json.loads(
            json.dumps(report, default=lambda o: o.__dict__ if hasattr(o, '__dict__') else str(o)))

        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(serializable_report, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ’¾ í‰ê°€ ë³´ê³ ì„œ ì €ì¥ ì™„ë£Œ: {filename}")

    def print_summary_report(self, report: Dict[str, Any]):
        """í‰ê°€ ìš”ì•½ ë³´ê³ ì„œ ì¶œë ¥"""
        print("\n" + "=" * 60)
        print("ğŸ“Š RAG ì‹œìŠ¤í…œ í‰ê°€ ìš”ì•½ ë³´ê³ ì„œ")
        print("=" * 60)

        avg = report['average_metrics']

        print("\n### ğŸ¯ ì¢…í•© í‰ê°€ ì§€í‘œ")
        print(f"  â€¢ ì˜ë¯¸ ìœ ì‚¬ë„: {avg['semantic_similarity']:.2%}")
        print(f"  â€¢ ë‹µë³€ ê´€ë ¨ì„±: {avg['answer_relevance']:.2%}")
        print(f"  â€¢ ì›ë¬¸ ì¶©ì‹¤ë„: {avg['faithfulness']:.2%}")
        print(f"  â€¢ ë‹µë³€ ì™„ì „ì„±: {avg['completeness']:.2%}")
        print(f"  â€¢ ì¼ê´€ì„±: {avg['consistency']:.2%}")
        print(f"  â€¢ í‰ê·  ì‘ë‹µì‹œê°„: {avg['response_time']:.2f}ì´ˆ")
        print(f"\n  â­ ì¢…í•© ì ìˆ˜: {avg['overall_score']:.2%}")

        print("\n### ğŸ“ˆ ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥")
        for cat, stats in report['category_analysis'].items():
            print(f"  â€¢ {cat}: {stats['avg_overall']:.2%} (í‰ê·  {stats['avg_response_time']:.2f}ì´ˆ)")

        assessment = report['system_assessment']
        print(f"\n### ğŸ† ì‹œìŠ¤í…œ í‰ê°€ ë“±ê¸‰: {assessment['overall_grade']}")

        if assessment['strengths']:
            print("\n### âœ… ê°•ì ")
            for strength in assessment['strengths']:
                print(f"  â€¢ {strength}")

        if assessment['weaknesses']:
            print("\n### âš ï¸ ê°œì„  í•„ìš” ì‚¬í•­")
            for weakness in assessment['weaknesses']:
                print(f"  â€¢ {weakness}")

        if assessment['recommendations']:
            print("\n### ğŸ’¡ ê°œì„  ì œì•ˆ")
            for rec in assessment['recommendations']:
                print(f"  â€¢ {rec}")

        print("\n" + "=" * 60)


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""

    # í‰ê°€í•  í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì •ì˜
    test_cases = [
        {
            'question': 'ì—”ì§„ì˜¤ì¼ êµì²´ ì£¼ê¸°ëŠ” ì–¼ë§ˆë‚˜ ë˜ë‚˜ìš”?',
            'category': 'ì •ë¹„',
            'expected': '15,000km ë˜ëŠ” 12ê°œì›”',
            'check_consistency': True
        },
        {
            'question': 'íƒ€ì´ì–´ ì ì • ê³µê¸°ì••ì€ ì–¼ë§ˆì¸ê°€ìš”?',
            'category': 'íƒ€ì´ì–´',
            'expected': '35psi'
        },
        {
            'question': 'ê²½ê³ ë“±ì´ ì¼œì¡Œì„ ë•Œ ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?',
            'category': 'ì•ˆì „',
            'expected': 'ì•ˆì „í•œ ê³³ì— ì •ì°¨'
        },
        {
            'question': 'ADAS ê¸°ëŠ¥ì„ ì„¤ì •í•˜ëŠ” ë°©ë²•ì€?',
            'category': 'ADAS',
            'expected': 'ì¸í¬í…Œì¸ë¨¼íŠ¸ ì‹œìŠ¤í…œì—ì„œ ì„¤ì •'
        },
        {
            'question': 'ë¸Œë ˆì´í¬ íŒ¨ë“œ êµì²´ ì‹œê¸°ëŠ”?',
            'category': 'ì •ë¹„',
            'expected': '30,000km'
        },
        {
            'question': 'ì™€ì´í¼ë¥¼ ì–´ë–»ê²Œ êµì²´í•˜ë‚˜ìš”?',
            'category': 'ì •ë¹„',
            'expected': 'ì™€ì´í¼ ì•”ì„ ë“¤ì–´ì˜¬ë ¤'
        }
    ]

    # í‰ê°€ ì‹œìŠ¤í…œ ì‹¤í–‰
    evaluator = RAGEvaluator()
    report = evaluator.evaluate_test_set(test_cases)

    # ë³´ê³ ì„œ ì¶œë ¥ ë° ì €ì¥
    evaluator.print_summary_report(report)
    evaluator.save_evaluation_report(report)

    # ë§ˆí¬ë‹¤ìš´ ìƒì„±
    print("\n### ğŸ“ í‰ê°€ ê²°ê³¼")
    print("```")
    print(f"ì¢…í•© ì ìˆ˜: {report['average_metrics']['overall_score']:.2%}")
    print(f"í‰ê·  ì‘ë‹µì‹œê°„: {report['average_metrics']['response_time']:.2f}ì´ˆ")
    print(f"ì‹œìŠ¤í…œ ë“±ê¸‰: {report['system_assessment']['overall_grade']}")
    print("```")


if __name__ == "__main__":
    # API í‚¤ í™•ì¸
    if not os.getenv("OPENAI_API_KEY"):
        print("âš ï¸ OPENAI_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”")
        api_key = input("API Key (sk-...): ").strip()
        if api_key:
            os.environ["OPENAI_API_KEY"] = api_key

    main()